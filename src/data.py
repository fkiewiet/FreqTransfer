from __future__ import annotations
from dataclasses import dataclass
from typing import Tuple, Optional
from pathlib import Path
import json

# Soft dependency guard (just like in ml.py)
try:
    import torch
    from torch.utils.data import Dataset
except Exception as _e:
    _TORCH_IMPORT_ERROR = _e
    torch = None      # type: ignore
    Dataset = object  # type: ignore

import numpy as np

from .config import GridSpec, PMLConfig, DATA_DIR
from .loads import build_load, RandomPointSource
from .operators import solve_with_pml_shell


# ---------------------------------------------------------------------
# Torch availability helper
# ---------------------------------------------------------------------
def _require_torch():
    if torch is None:
        raise ImportError(
            "PyTorch is required for this functionality but could not be imported. "
            f"Original error: {_TORCH_IMPORT_ERROR}"
        )


# ---------------------------------------------------------------------
# Basic utility: complex -> 2-channel real
# ---------------------------------------------------------------------
def np_complex_to_2ch(u: np.ndarray) -> np.ndarray:
    """
    Convert a complex-valued field u (H,W) or (N,H,W) to a real-valued
    array with shape (N, 2, H, W) or (1, 2, H, W).
    """
    if u.ndim == 2:
        u = u[None, ...]  # (1,H,W)

    real = np.real(u)
    imag = np.imag(u)
    return np.stack([real, imag], axis=1)  # (N,2,H,W)


# =============================================================================
# Datasets based on PDE solves
# =============================================================================

class HelmholtzFreqTransferDataset(Dataset):
    """
    Dataset of frequency-transfer pairs (u_src, u_tgt) generated by
    direct solves with a PML shell, for the same random RHS.

    Each item:
        u_src: (2, H, W) torch.float32, solution at omega_src
        u_tgt: (2, H, W) torch.float32, solution at omega_tgt
    """
    def __init__(
        self,
        grid: GridSpec,
        pml: PMLConfig,
        omega_src: float,
        omega_tgt: float,
        seeds,
        omega_to_k,
    ):
        _require_torch()
        self.grid = grid
        self.pml = pml
        self.omega_src = omega_src
        self.omega_tgt = omega_tgt
        self.seeds = list(seeds)
        self.omega_to_k = omega_to_k

    def _solve_at_omega(self, omega: float, seed: int) -> np.ndarray:
        k = float(self.omega_to_k(omega))
        rhs_spec = RandomPointSource(seed=seed)
        b = build_load(rhs_spec, self.grid)        # (N_phys,)
        u_phys, _, _ = solve_with_pml_shell(self.grid, k, b, self.pml)
        return u_phys.reshape(self.grid.shape)     # (H,W) complex

    def _solve_pair(self, idx: int):
        seed = self.seeds[idx]
        u_src_c = self._solve_at_omega(self.omega_src, seed)
        u_tgt_c = self._solve_at_omega(self.omega_tgt, seed)

        u_src_2ch = np_complex_to_2ch(u_src_c)[0]  # (2,H,W)
        u_tgt_2ch = np_complex_to_2ch(u_tgt_c)[0]  # (2,H,W)

        return (
            torch.from_numpy(u_src_2ch).float(),
            torch.from_numpy(u_tgt_2ch).float(),
        )

    def __getitem__(self, idx: int):
        return self._solve_pair(idx)

    def __len__(self) -> int:
        return len(self.seeds)


# ---------------------------------------------------------------------
# Cached datasets (X/Y/meta on disk) + builder
# ---------------------------------------------------------------------

class PrecomputedFreqDataset(Dataset):
    """
    Dataset that serves precomputed (u_src, u_tgt) from X.npy/Y.npy on disk.

    Each item:
      x: (2, H, W) float32  -> source field [Re, Im]
      y: (2, H, W) float32  -> target field [Re, Im]
    """
    def __init__(self, root: Path):
        _require_torch()
        self.root = Path(root)
        self.X = np.load(self.root / "X.npy")  # (N,2,H,W)
        self.Y = np.load(self.root / "Y.npy")  # (N,2,H,W)
        with open(self.root / "meta.json", "r") as f:
            self.meta = json.load(f)

    def __len__(self) -> int:
        return self.X.shape[0]

    def __getitem__(self, idx: int):
        x = torch.from_numpy(self.X[idx]).float()
        y = torch.from_numpy(self.Y[idx]).float()
        return x, y


def _freq_dataset_name(
    grid: GridSpec,
    pml: PMLConfig,
    omega_src: float,
    omega_tgt: float,
    N_samples: int,
) -> str:
    H, W = grid.shape
    name = (
        f"wsrc{omega_src:.3f}_wtgt{omega_tgt:.3f}"
        f"_N{N_samples}_"
        f"grid{H}x{W}_"
        f"pmlT{pml.thickness}_m{pml.m}_sig{pml.sigma_max:.2f}"
    )
    return name.replace(".", "p")


def get_freq_dataset(
    grid: GridSpec,
    pml: PMLConfig,
    omega_src: float,
    omega_tgt: float,
    N_samples: int,
    omega_to_k=None,
    cache_root: Path | None = None,
    overwrite: bool = False,
) -> Dataset:
    """
    Build or load a frequency-transfer dataset, with automatic caching.

    Returns a PrecomputedFreqDataset that reads (u_src, u_tgt) from disk.
    """
    _require_torch()

    if omega_to_k is None:
        def omega_to_k(omega: float) -> float:
            c = 1.0
            return float(omega / c)

    if cache_root is None:
        cache_root = DATA_DIR / "freq_transfer_cached"

    cache_root = Path(cache_root)
    cache_root.mkdir(parents=True, exist_ok=True)

    name = _freq_dataset_name(grid, pml, omega_src, omega_tgt, N_samples)
    ds_dir = cache_root / name

    X_path = ds_dir / "X.npy"
    Y_path = ds_dir / "Y.npy"
    meta_path = ds_dir / "meta.json"

    # Try to load existing cache
    if (
        ds_dir.exists()
        and X_path.exists()
        and Y_path.exists()
        and meta_path.exists()
        and not overwrite
    ):
        try:
            print(f"[get_freq_dataset] Loading cached dataset from: {ds_dir}")
            ds = PrecomputedFreqDataset(ds_dir)
            print(
                f"  Loaded N={len(ds)} samples, "
                f"ω_src={ds.meta['omega_src']}, ω_tgt={ds.meta['omega_tgt']}"
            )
            return ds
        except Exception as e:
            print(
                f"[get_freq_dataset] WARNING: failed to load cache at {ds_dir}: {e}\n"
                "  Falling back to recomputation and overwriting this directory."
            )
            overwrite = True

    # Otherwise: recompute
    print(f"[get_freq_dataset] Generating new dataset: {name}")
    ds_dir.mkdir(parents=True, exist_ok=True)

    rng = np.random.default_rng(12345)
    seeds = [int(s) for s in rng.integers(0, 1_000_000, size=N_samples)]

    online_ds = HelmholtzFreqTransferDataset(
        grid=grid,
        pml=pml,
        omega_src=omega_src,
        omega_tgt=omega_tgt,
        seeds=seeds,
        omega_to_k=omega_to_k,
    )

    u_src0, u_tgt0 = online_ds[0]
    C, H, W = u_src0.shape
    X = np.zeros((N_samples, C, H, W), dtype=np.float32)
    Y = np.zeros((N_samples, C, H, W), dtype=np.float32)

    for i in range(N_samples):
        if i % max(1, N_samples // 10) == 0:
            print(f"  solving sample {i}/{N_samples} ...")
        u_src, u_tgt = online_ds[i]
        X[i] = u_src.numpy()
        Y[i] = u_tgt.numpy()

    np.save(X_path, X)
    np.save(Y_path, Y)

    meta = {
        "omega_src": float(omega_src),
        "omega_tgt": float(omega_tgt),
        "grid_shape": [int(s) for s in grid.shape],
        "grid_lengths": [float(L) for L in grid.lengths],
        "pml": {
            "thickness": int(pml.thickness),
            "m": int(pml.m),
            "sigma_max": float(pml.sigma_max),
        },
        "N_samples": int(N_samples),
        "seeds": [int(s) for s in seeds],
    }
    with meta_path.open("w") as f:
        json.dump(meta, f, indent=2)

    print(f"[get_freq_dataset] Saved dataset to {ds_dir}")
    print(f"  X.npy shape: {X.shape}")
    print(f"  Y.npy shape: {Y.shape}")

    return PrecomputedFreqDataset(ds_dir)


# =============================================================================
# Normalisation wrappers
# =============================================================================

class AmpNormWrapper(Dataset):
    """
    Wraps a dataset of (u_src, u_tgt) tensors and rescales each sample so that
    both u_src and u_tgt have unit L2 norm (approximately).
    """
    def __init__(self, base_ds: Dataset):
        _require_torch()
        self.ds = base_ds

    def __len__(self) -> int:
        return len(self.ds)

    def __getitem__(self, idx: int):
        u_src, u_tgt = self.ds[idx]
        amp_src = torch.linalg.vector_norm(u_src)
        amp_tgt = torch.linalg.vector_norm(u_tgt)
        u_src = u_src / (amp_src + 1e-8)
        u_tgt = u_tgt / (amp_tgt + 1e-8)
        return u_src, u_tgt


def compute_input_stats(ds: Dataset, max_samples: int = 200) -> Tuple[torch.Tensor, torch.Tensor]:
    """
    Compute per-channel mean and std of u_src over the dataset.
    """
    _require_torch()
    acc = []
    n = min(len(ds), max_samples)
    for i in range(n):
        u_src, _ = ds[i]
        acc.append(u_src.view(u_src.shape[0], -1))
    acc = torch.cat(acc, dim=1)
    mean = acc.mean(dim=1)
    std = acc.std(dim=1) + 1e-8
    return mean, std


class StdNormWrapper(Dataset):
    """
    Wraps a dataset of (u_src, u_tgt) tensors and applies per-channel
    standardisation using provided mean and std.
    """
    def __init__(self, base_ds: Dataset, mean: torch.Tensor, std: torch.Tensor):
        _require_torch()
        self.ds = base_ds
        self.mean = mean.view(-1, 1, 1)
        self.std = std.view(-1, 1, 1)

    def __len__(self) -> int:
        return len(self.ds)

    def __getitem__(self, idx: int):
        u_src, u_tgt = self.ds[idx]
        u_src = (u_src - self.mean) / self.std
        u_tgt = (u_tgt - self.mean) / self.std
        return u_src, u_tgt


class ScaleWrapper(Dataset):
    """
    Multiply both input and target by a constant scaling factor.
    Useful because Helmholtz solutions typically have amplitude ~1e-4,
    which makes learning unstable unless we lift them to O(1).

    Example:
        scaled_ds = ScaleWrapper(raw_ds, factor=1e4)
    """
    def __init__(self, base_ds: Dataset, factor: float):
        _require_torch()
        self.ds = base_ds
        self.factor = float(factor)

    def __len__(self):
        return len(self.ds)

    def __getitem__(self, idx):
        x, y = self.ds[idx]
        return x * self.factor, y * self.factor



class CoordWrapper(Dataset):
    """
    Wrap a dataset of (u_src, u_tgt) and augment the input with
    two extra channels containing the (x, y) coordinates on the grid.

    After wrapping:
        input: (4, H, W) = [Re(u_src), Im(u_src), x_coord, y_coord]
        target: (2, H, W) = [Re(u_tgt), Im(u_tgt)]
    """
    def __init__(self, base_ds: Dataset, grid: GridSpec, normalise: bool = True):
        _require_torch()
        self.ds = base_ds

        H, W = grid.shape
        Ly, Lx = grid.lengths  # note: (Ly, Lx) if you follow your earlier convention

        # Build coordinate grid
        ys = torch.linspace(0.0, Ly, H)
        xs = torch.linspace(0.0, Lx, W)
        yy, xx = torch.meshgrid(ys, xs, indexing="ij")  # shape (H, W)

        if normalise:
            if Ly != 0:
                yy = yy / Ly  # 0..1
            if Lx != 0:
                xx = xx / Lx  # 0..1

        # coords: (2, H, W) -> [x, y]
        self.coords = torch.stack([xx, yy], dim=0).float()

    def __len__(self) -> int:
        return len(self.ds)

    def __getitem__(self, idx: int):
        u_src, u_tgt = self.ds[idx]       # u_src: (2, H, W)
        # Make sure coords is on same device/dtype at training time; here we keep it CPU/float32
        x_in = torch.cat([u_src, self.coords.to(u_src.device, u_src.dtype)], dim=0)
        return x_in, u_tgt
    

#extra wrapper for identity mapping (u)


class IdentityWrapper(Dataset):
    """Return (u, u) instead of (x, u)."""
    def __init__(self, base_ds):
        self.ds = base_ds
    def __len__(self):
        return len(self.ds)
    def __getitem__(self, idx):
        x, y = self.ds[idx]
        return y, y  # treat solution as both input and target
    

class OmegaCoordWrapper(Dataset):
    """
    Wrap a dataset of (u_src, u_tgt) and augment the input with:
        - (x, y) coordinate channels
        -  scalar channels for omega_src and omega_tgt

    After wrapping:
        input: (6, H, W) =
            [Re(u_src), Im(u_src), x_coord, y_coord, omega_src, omega_tgt]
        target: (2, H, W) = [Re(u_tgt), Im(u_tgt)]

    omega_src, omega_tgt are taken as *global* for the dataset (the ones
    you used to generate the cached dataset).
    """
    def __init__(
        self,
        base_ds: Dataset,
        grid: GridSpec,
        omega_src: float,
        omega_tgt: float,
        normalise_coords: bool = True,
        normalise_omega: bool = True,
    ):
        _require_torch()
        self.ds = base_ds
        self.omega_src = float(omega_src)
        self.omega_tgt = float(omega_tgt)

        H, W = grid.shape
        Ly, Lx = grid.lengths  # (Ly, Lx)

        # --- coordinate grid ---
        ys = torch.linspace(0.0, Ly, H)
        xs = torch.linspace(0.0, Lx, W)
        yy, xx = torch.meshgrid(ys, xs, indexing="ij")  # (H, W)

        if normalise_coords:
            if Ly != 0:
                yy = yy / Ly   # 0..1
            if Lx != 0:
                xx = xx / Lx   # 0..1

        # coords: (2, H, W)
        self.coords = torch.stack([xx, yy], dim=0).float()

        # --- omega channels ---
        # Start as scalar; we turn into (H, W) in __getitem__ to keep it simple.
        self.normalise_omega = normalise_omega

    def __len__(self) -> int:
        return len(self.ds)

    def __getitem__(self, idx: int):
        # base_ds returns (u_src, u_tgt) with shapes (2, H, W)
        u_src, u_tgt = self.ds[idx]
        _, H, W = u_src.shape

        # coords: (2, H, W)
        coords = self.coords

        # build omega_src and omega_tgt maps of shape (H, W)
        ωs = torch.full((H, W), self.omega_src, dtype=torch.float32)
        ωt = torch.full((H, W), self.omega_tgt, dtype=torch.float32)

        if self.normalise_omega:
            # simple normalisation: divide by max(omega_src, omega_tgt)
            ω_max = max(abs(self.omega_src), abs(self.omega_tgt), 1.0)
            ωs = ωs / ω_max
            ωt = ωt / ω_max

        omega_ch = torch.stack([ωs, ωt], dim=0)  # (2, H, W)

        # final input: (6, H, W)
        x_in = torch.cat([u_src, coords, omega_ch], dim=0)
        return x_in, u_tgt


class DeltaTargetWrapper(Dataset):
    """
    Wraps a dataset of (u_src, u_tgt) and turns the target into
        Δu = u_tgt - u_src.

    This is meant to be used *before* any input-augmentation wrapper
    like CoordWrapper, e.g.:

        base      : (u_src, u_tgt)
        delta_ds  = DeltaTargetWrapper(base)          -> (u_src, Δu)
        full_ds   = CoordWrapper(delta_ds, grid, ...) -> ([u_src, x, y], Δu)
    """
    def __init__(self, base_ds: Dataset):
        _require_torch()
        self.ds = base_ds

    def __len__(self) -> int:
        return len(self.ds)

    def __getitem__(self, idx: int):
        u_src, u_tgt = self.ds[idx]    # both (2, H, W)
        delta = u_tgt - u_src          # (2, H, W)
        return u_src, delta
    


class GainWrapper(Dataset):
    """
    Wrap (u_src, u_tgt) and produce:
      x : (C_in, H, W)   = [Re(u_src), Im(u_src), (optional coords/omegas...)]
      y : (2, H, W)      = [log |G|, Δφ], where G = u_tgt / u_src.
    We mask locations where |u_src| is tiny to avoid division blow-ups.
    """
    def __init__(
        self,
        base_ds: Dataset,
        add_coords: bool,
        grid: GridSpec | None = None,
        omega_src: float | None = None,
        omega_tgt: float | None = None,
        eps: float = 1e-8,
    ):
        _require_torch()
        self.ds = base_ds
        self.eps = eps
        self.add_coords = add_coords

        # precompute coords if requested
        if add_coords:
            assert grid is not None
            H, W = grid.shape
            Ly, Lx = grid.lengths
            ys = torch.linspace(0.0, Ly, H)
            xs = torch.linspace(0.0, Lx, W)
            yy, xx = torch.meshgrid(ys, xs, indexing="ij")
            yy = yy / (Ly + 1e-12)
            xx = xx / (Lx + 1e-12)
            self.coords = torch.stack([xx, yy], dim=0).float()  # (2,H,W)
        else:
            self.coords = None

        # encode omegas as scalars (broadcast later)
        self.omega_src = float(omega_src) if omega_src is not None else None
        self.omega_tgt = float(omega_tgt) if omega_tgt is not None else None

    def __len__(self):
        return len(self.ds)

    def __getitem__(self, idx: int):
        u_src, u_tgt = self.ds[idx]          # (2,H,W) each
        H, W = u_src.shape[-2:]

        # complex fields
        u_src_c = u_src[0] + 1j * u_src[1]
        u_tgt_c = u_tgt[0] + 1j * u_tgt[1]

        mag_src = torch.abs(u_src_c)
        mag_tgt = torch.abs(u_tgt_c)

        # avoid division by tiny |u_src|
        mask = mag_src > self.eps
        G = torch.zeros_like(u_src_c)
        G[mask] = (u_tgt_c[mask] / (u_src_c[mask] + 0j))

        # target channels: log|G|, Δφ ∈ [-π,π]
        logG = torch.zeros_like(mag_src)
        logG[mask] = torch.log(torch.abs(G[mask]) + 1e-12)

        dphi = torch.angle(G)  # already wrapped

        y = torch.stack([logG, dphi], dim=0)  # (2,H,W)

        # build input: [Re(u_src), Im(u_src), (coords?), (omegas?)]
        x_ch = [u_src[0], u_src[1]]

        if self.add_coords:
            x_ch.extend([self.coords[0], self.coords[1]])

        if self.omega_src is not None and self.omega_tgt is not None:
            # normalise a bit
            w_src = torch.full((H, W), self.omega_src / 20.0)
            w_tgt = torch.full((H, W), self.omega_tgt / 20.0)
            x_ch.extend([w_src, w_tgt])

        x = torch.stack(x_ch, dim=0).float()  # (C_in,H,W)

        return x, y
    

class OmegaChannelWrapper(torch.utils.data.Dataset):
    """
    Takes a base dataset (x,y) with x: [C,H,W] and adds two constant
    channels with ω_src and ω_tgt, giving [C+2,H,W].
    """
    def __init__(self, base_ds, omega_src: float, omega_tgt: float):
        self.base_ds = base_ds
        self.omega_src = float(omega_src)
        self.omega_tgt = float(omega_tgt)

    def __len__(self):
        return len(self.base_ds)

    def __getitem__(self, idx):
        x, y = self.base_ds[idx]          # x: [C,H,W], y: [2,H,W]
        H, W = x.shape[-2], x.shape[-1]
        w_src_chan = torch.full((1, H, W), self.omega_src, dtype=x.dtype)
        w_tgt_chan = torch.full((1, H, W), self.omega_tgt, dtype=x.dtype)
        x_aug = torch.cat([x, w_src_chan, w_tgt_chan], dim=0)  # [C+2,H,W]
        return x_aug, y



__all__ = [
    "np_complex_to_2ch",
    "HelmholtzFreqTransferDataset",
    "PrecomputedFreqDataset",
    "get_freq_dataset",
    "AmpNormWrapper",
    "StdNormWrapper",
    "compute_input_stats",
    "ScaleWrapper",      
    "CoordWrapper",
    "IdentityWrapper",
    "OmegaCoordWrapper",
    "DeltaTargetWrapper",
    "GainWrapper",
    "OmegaChannelWrapper",
]
